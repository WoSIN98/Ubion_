{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Participant 302: Files: ['302_processed_1.wav', '302_processed_2.wav', '302_processed_3.wav', '302_processed_final.wav']\n",
      "Label: 0\n",
      "Participant 307: Files: ['307_processed_1.wav', '307_processed_10.wav', '307_processed_11.wav', '307_processed_12.wav', '307_processed_13.wav', '307_processed_14.wav', '307_processed_15.wav', '307_processed_2.wav', '307_processed_3.wav', '307_processed_4.wav', '307_processed_5.wav', '307_processed_6.wav', '307_processed_7.wav', '307_processed_8.wav', '307_processed_9.wav', '307_processed_final.wav']\n",
      "Label: 0\n",
      "Participant 331: Files: ['331_processed_1.wav', '331_processed_2.wav', '331_processed_3.wav', '331_processed_4.wav', '331_processed_5.wav', '331_processed_6.wav', '331_processed_final.wav']\n",
      "Label: 0\n",
      "Participant 335: Files: ['335_processed_1.wav', '335_processed_2.wav', '335_processed_3.wav', '335_processed_4.wav', '335_processed_5.wav', '335_processed_6.wav', '335_processed_7.wav', '335_processed_final.wav']\n",
      "Label: 1\n",
      "Participant 346: Files: ['346_processed_1.wav', '346_processed_10.wav', '346_processed_11.wav', '346_processed_2.wav', '346_processed_3.wav', '346_processed_4.wav', '346_processed_5.wav', '346_processed_6.wav', '346_processed_7.wav', '346_processed_8.wav', '346_processed_9.wav', '346_processed_final.wav']\n",
      "Label: 1\n",
      "Participant 367: Files: ['367_processed_1.wav', '367_processed_10.wav', '367_processed_11.wav', '367_processed_12.wav', '367_processed_13.wav', '367_processed_14.wav', '367_processed_15.wav', '367_processed_16.wav', '367_processed_17.wav', '367_processed_2.wav', '367_processed_3.wav', '367_processed_4.wav', '367_processed_5.wav', '367_processed_6.wav', '367_processed_7.wav', '367_processed_8.wav', '367_processed_9.wav', '367_processed_final.wav']\n",
      "Label: 1\n",
      "Participant 377: Files: ['377_processed_1.wav', '377_processed_10.wav', '377_processed_11.wav', '377_processed_12.wav', '377_processed_13.wav', '377_processed_2.wav', '377_processed_3.wav', '377_processed_4.wav', '377_processed_5.wav', '377_processed_6.wav', '377_processed_7.wav', '377_processed_8.wav', '377_processed_9.wav', '377_processed_final.wav']\n",
      "Label: 1\n",
      "Participant 381: Files: ['381_processed_1.wav', '381_processed_2.wav', '381_processed_3.wav', '381_processed_4.wav', '381_processed_5.wav', '381_processed_6.wav', '381_processed_7.wav', '381_processed_final.wav']\n",
      "Label: 1\n",
      "Participant 382: Files: ['382_processed_1.wav', '382_processed_2.wav', '382_processed_3.wav', '382_processed_4.wav', '382_processed_5.wav', '382_processed_final.wav']\n",
      "Label: 0\n",
      "Participant 388: Files: ['388_processed_1.wav', '388_processed_2.wav', '388_processed_final.wav']\n",
      "Label: 1\n",
      "Participant 389: Files: ['389_processed_1.wav', '389_processed_2.wav', '389_processed_3.wav', '389_processed_final.wav']\n",
      "Label: 1\n",
      "Participant 390: Files: ['390_processed_1.wav', '390_processed_10.wav', '390_processed_11.wav', '390_processed_2.wav', '390_processed_3.wav', '390_processed_4.wav', '390_processed_5.wav', '390_processed_6.wav', '390_processed_7.wav', '390_processed_8.wav', '390_processed_9.wav', '390_processed_final.wav']\n",
      "Label: 0\n",
      "Participant 395: Files: ['395_processed_1.wav', '395_processed_2.wav', '395_processed_3.wav', '395_processed_4.wav', '395_processed_5.wav', '395_processed_6.wav', '395_processed_7.wav', '395_processed_final.wav']\n",
      "Label: 0\n",
      "Participant 403: Files: ['403_processed_1.wav', '403_processed_2.wav', '403_processed_3.wav', '403_processed_4.wav', '403_processed_5.wav', '403_processed_6.wav', '403_processed_7.wav', '403_processed_final.wav']\n",
      "Label: 0\n",
      "Participant 404: Files: ['404_processed_1.wav', '404_processed_2.wav', '404_processed_3.wav', '404_processed_4.wav', '404_processed_5.wav', '404_processed_6.wav', '404_processed_7.wav', '404_processed_8.wav', '404_processed_final.wav']\n",
      "Label: 0\n",
      "Participant 406: Files: ['406_processed_1.wav', '406_processed_2.wav', '406_processed_3.wav', '406_processed_4.wav', '406_processed_5.wav', '406_processed_final.wav']\n",
      "Label: 0\n",
      "Participant 413: Files: ['413_processed_1.wav', '413_processed_2.wav', '413_processed_3.wav', '413_processed_4.wav', '413_processed_5.wav', '413_processed_6.wav', '413_processed_7.wav', '413_processed_8.wav', '413_processed_final.wav']\n",
      "Label: 1\n",
      "Participant 417: Files: ['417_processed_1.wav', '417_processed_2.wav', '417_processed_3.wav', '417_processed_4.wav', '417_processed_5.wav', '417_processed_6.wav', '417_processed_7.wav', '417_processed_final.wav']\n",
      "Label: 0\n",
      "Participant 418: Files: ['418_processed_1.wav', '418_processed_10.wav', '418_processed_2.wav', '418_processed_3.wav', '418_processed_4.wav', '418_processed_5.wav', '418_processed_6.wav', '418_processed_7.wav', '418_processed_8.wav', '418_processed_9.wav', '418_processed_final.wav']\n",
      "Label: 1\n",
      "Participant 420: Files: ['420_processed_1.wav', '420_processed_2.wav', '420_processed_3.wav', '420_processed_4.wav', '420_processed_5.wav', '420_processed_6.wav', '420_processed_final.wav']\n",
      "Label: 0\n",
      "Participant 422: Files: ['422_processed_1.wav', '422_processed_10.wav', '422_processed_11.wav', '422_processed_12.wav', '422_processed_2.wav', '422_processed_3.wav', '422_processed_4.wav', '422_processed_5.wav', '422_processed_6.wav', '422_processed_7.wav', '422_processed_8.wav', '422_processed_9.wav', '422_processed_final.wav']\n",
      "Label: 1\n",
      "Participant 436: Files: ['436_processed_1.wav', '436_processed_2.wav', '436_processed_3.wav', '436_processed_4.wav', '436_processed_5.wav', '436_processed_final.wav']\n",
      "Label: 0\n",
      "Participant 439: Files: ['439_processed_1.wav', '439_processed_10.wav', '439_processed_11.wav', '439_processed_12.wav', '439_processed_13.wav', '439_processed_14.wav', '439_processed_15.wav', '439_processed_2.wav', '439_processed_3.wav', '439_processed_4.wav', '439_processed_5.wav', '439_processed_6.wav', '439_processed_7.wav', '439_processed_8.wav', '439_processed_9.wav', '439_processed_final.wav']\n",
      "Label: 0\n",
      "Participant 440: Files: ['440_processed_1.wav', '440_processed_10.wav', '440_processed_11.wav', '440_processed_2.wav', '440_processed_3.wav', '440_processed_4.wav', '440_processed_5.wav', '440_processed_6.wav', '440_processed_7.wav', '440_processed_8.wav', '440_processed_9.wav', '440_processed_final.wav']\n",
      "Label: 1\n",
      "Participant 451: Files: ['451_processed_1.wav', '451_processed_10.wav', '451_processed_2.wav', '451_processed_3.wav', '451_processed_4.wav', '451_processed_5.wav', '451_processed_6.wav', '451_processed_7.wav', '451_processed_8.wav', '451_processed_9.wav', '451_processed_final.wav']\n",
      "Label: 0\n",
      "Participant 458: Files: ['458_processed_1.wav', '458_processed_2.wav', '458_processed_3.wav', '458_processed_4.wav', '458_processed_5.wav', '458_processed_6.wav', '458_processed_7.wav', '458_processed_8.wav', '458_processed_9.wav', '458_processed_final.wav']\n",
      "Label: 0\n",
      "Participant 472: Files: ['472_processed_1.wav', '472_processed_2.wav', '472_processed_3.wav', '472_processed_4.wav', '472_processed_5.wav', '472_processed_6.wav', '472_processed_final.wav']\n",
      "Label: 0\n",
      "Participant 476: Files: ['476_processed_1.wav', '476_processed_2.wav', '476_processed_3.wav', '476_processed_final.wav']\n",
      "Label: 0\n",
      "Participant 477: Files: ['477_processed_1.wav', '477_processed_10.wav', '477_processed_11.wav', '477_processed_12.wav', '477_processed_13.wav', '477_processed_2.wav', '477_processed_3.wav', '477_processed_4.wav', '477_processed_5.wav', '477_processed_6.wav', '477_processed_7.wav', '477_processed_8.wav', '477_processed_9.wav', '477_processed_final.wav']\n",
      "Label: 0\n",
      "Participant 482: Files: ['482_processed_1.wav', '482_processed_2.wav', '482_processed_3.wav', '482_processed_4.wav', '482_processed_5.wav', '482_processed_6.wav', '482_processed_7.wav', '482_processed_8.wav', '482_processed_final.wav']\n",
      "Label: 0\n",
      "Participant 483: Files: ['483_processed_1.wav', '483_processed_10.wav', '483_processed_11.wav', '483_processed_12.wav', '483_processed_13.wav', '483_processed_14.wav', '483_processed_2.wav', '483_processed_3.wav', '483_processed_4.wav', '483_processed_5.wav', '483_processed_6.wav', '483_processed_7.wav', '483_processed_8.wav', '483_processed_9.wav', '483_processed_final.wav']\n",
      "Label: 1\n",
      "Participant 484: Files: ['484_processed_1.wav', '484_processed_2.wav', '484_processed_3.wav', '484_processed_4.wav', '484_processed_5.wav', '484_processed_6.wav', '484_processed_7.wav', '484_processed_8.wav', '484_processed_9.wav', '484_processed_final.wav']\n",
      "Label: 0\n",
      "Participant 489: Files: ['489_processed_1.wav', '489_processed_2.wav', '489_processed_final.wav']\n",
      "Label: 0\n",
      "Participant 490: Files: ['490_processed_1.wav', '490_processed_2.wav', '490_processed_3.wav', '490_processed_final.wav']\n",
      "Label: 0\n",
      "Participant 492: Files: ['492_processed_1.wav', '492_processed_2.wav', '492_processed_3.wav', '492_processed_4.wav', '492_processed_5.wav', '492_processed_6.wav', '492_processed_7.wav', '492_processed_8.wav', '492_processed_final.wav']\n",
      "Label: 0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('../split/dev_split_Depression.csv')\n",
    "\n",
    "# 참가자 ID와 PHQ8_Binary 컬럼만 추출하여 새로운 데이터프레임 생성\n",
    "participant_labels = data[['Participant_ID', 'PHQ8_Binary']].set_index('Participant_ID')\n",
    "\n",
    "# 참가자 ID와 wav 파일명을 매핑하기 위한 딕셔너리 생성\n",
    "wav_files = {}\n",
    "wav_directory = '../data/reduce/dev'  # wav 파일이 위치한 디렉토리\n",
    "\n",
    "# 디렉토리 내 모든 파일 목록을 검색\n",
    "for filename in os.listdir(wav_directory):\n",
    "    if filename.endswith('.wav'):\n",
    "        # 파일명에서 참가자 ID 추출 (예: '302_processed_1.wav'에서 '302' 추출)\n",
    "        participant_id = int(filename.split('_')[0])\n",
    "        if participant_id in wav_files:\n",
    "            wav_files[participant_id].append(filename)\n",
    "        else:\n",
    "            wav_files[participant_id] = [filename]\n",
    "\n",
    "# 각 참가자 ID에 대해 해당하는 wav 파일 리스트 출력\n",
    "for participant_id, files in wav_files.items():\n",
    "    print(f\"Participant {participant_id}: Files: {files}\")\n",
    "    print(f\"Label: {participant_labels.loc[participant_id, 'PHQ8_Binary']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor, Trainer, TrainingArguments\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "from datasets import Dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 오디오 파일 처리 및 데이터셋 준비\n",
    "오디오 파일을 로드하고, 모델이 입력으로 사용할 수 있도록 특징을 추출하는 함수를 정의합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading feature extractor configuration file https://huggingface.co/kresnik/wav2vec2-large-xlsr-korean/resolve/main/preprocessor_config.json from cache at C:\\Users\\Jws/.cache\\huggingface\\transformers\\27306b13697cb64bc43e0f1d31f53f092d15df6edd9a31f1c02e47b1602e99d8.bbc1eb890a39c82e710a893223b8452ac5b78e8b57083b2f893aa7dc59d4ed69\n",
      "Feature extractor Wav2Vec2FeatureExtractor {\n",
      "  \"do_normalize\": true,\n",
      "  \"feature_extractor_type\": \"Wav2Vec2FeatureExtractor\",\n",
      "  \"feature_size\": 1,\n",
      "  \"padding_side\": \"right\",\n",
      "  \"padding_value\": 0.0,\n",
      "  \"return_attention_mask\": true,\n",
      "  \"sampling_rate\": 16000\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/kresnik/wav2vec2-large-xlsr-korean/resolve/main/vocab.json from cache at C:\\Users\\Jws/.cache\\huggingface\\transformers\\3124f16a7fe8ebc39e54f235f7839aa5a221e8c1d48887c5ddef37a61d735d41.aaf4866cca866da8b42867b565d7bb952a198499d25daa80e0695b7937811d9d\n",
      "loading file https://huggingface.co/kresnik/wav2vec2-large-xlsr-korean/resolve/main/tokenizer_config.json from cache at C:\\Users\\Jws/.cache\\huggingface\\transformers\\0717311cf5c142e6a2c776f3d4d3625a0de5d28515f6561b3d7b93bc4ccd035e.afa46706992ff886bba67f711d178e04ad6c052142e508c64e8c10b514a2e5f4\n",
      "loading file https://huggingface.co/kresnik/wav2vec2-large-xlsr-korean/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/kresnik/wav2vec2-large-xlsr-korean/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/kresnik/wav2vec2-large-xlsr-korean/resolve/main/tokenizer.json from cache at None\n",
      "loading configuration file https://huggingface.co/kresnik/wav2vec2-large-xlsr-korean/resolve/main/config.json from cache at C:\\Users\\Jws/.cache\\huggingface\\transformers\\d33b22e404661c9d64ae19906d25af36ff93cb444de2ca532ee5f68ebe79440d.de221d5718b1871c71ff30c71be8d85c0e7038148801458ad4ee9b4a4a6e92e7\n",
      "Model config Wav2Vec2Config {\n",
      "  \"_name_or_path\": \"facebook/wav2vec2-large-xlsr-53\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"adapter_kernel_size\": 3,\n",
      "  \"adapter_stride\": 2,\n",
      "  \"add_adapter\": false,\n",
      "  \"apply_spec_augment\": true,\n",
      "  \"architectures\": [\n",
      "    \"Wav2Vec2ForCTC\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"classifier_proj_size\": 256,\n",
      "  \"codevector_dim\": 768,\n",
      "  \"contrastive_logits_temperature\": 0.1,\n",
      "  \"conv_bias\": true,\n",
      "  \"conv_dim\": [\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512\n",
      "  ],\n",
      "  \"conv_kernel\": [\n",
      "    10,\n",
      "    3,\n",
      "    3,\n",
      "    3,\n",
      "    3,\n",
      "    2,\n",
      "    2\n",
      "  ],\n",
      "  \"conv_stride\": [\n",
      "    5,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2\n",
      "  ],\n",
      "  \"ctc_loss_reduction\": \"mean\",\n",
      "  \"ctc_zero_infinity\": false,\n",
      "  \"diversity_loss_weight\": 0.1,\n",
      "  \"do_stable_layer_norm\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"feat_extract_activation\": \"gelu\",\n",
      "  \"feat_extract_dropout\": 0.0,\n",
      "  \"feat_extract_norm\": \"layer\",\n",
      "  \"feat_proj_dropout\": 0.0,\n",
      "  \"feat_quantizer_dropout\": 0.0,\n",
      "  \"final_dropout\": 0.0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"layerdrop\": 0.1,\n",
      "  \"mask_channel_length\": 10,\n",
      "  \"mask_channel_min_space\": 1,\n",
      "  \"mask_channel_other\": 0.0,\n",
      "  \"mask_channel_prob\": 0.0,\n",
      "  \"mask_channel_selection\": \"static\",\n",
      "  \"mask_feature_length\": 10,\n",
      "  \"mask_feature_min_masks\": 0,\n",
      "  \"mask_feature_prob\": 0.0,\n",
      "  \"mask_time_length\": 10,\n",
      "  \"mask_time_min_masks\": 2,\n",
      "  \"mask_time_min_space\": 1,\n",
      "  \"mask_time_other\": 0.0,\n",
      "  \"mask_time_prob\": 0.05,\n",
      "  \"mask_time_selection\": \"static\",\n",
      "  \"model_type\": \"wav2vec2\",\n",
      "  \"num_adapter_layers\": 3,\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_codevector_groups\": 2,\n",
      "  \"num_codevectors_per_group\": 320,\n",
      "  \"num_conv_pos_embedding_groups\": 16,\n",
      "  \"num_conv_pos_embeddings\": 128,\n",
      "  \"num_feat_extract_layers\": 7,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_negatives\": 100,\n",
      "  \"output_hidden_size\": 1024,\n",
      "  \"pad_token_id\": 1204,\n",
      "  \"proj_codevector_dim\": 768,\n",
      "  \"tdnn_dilation\": [\n",
      "    1,\n",
      "    2,\n",
      "    3,\n",
      "    1,\n",
      "    1\n",
      "  ],\n",
      "  \"tdnn_dim\": [\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    1500\n",
      "  ],\n",
      "  \"tdnn_kernel\": [\n",
      "    5,\n",
      "    3,\n",
      "    3,\n",
      "    1,\n",
      "    1\n",
      "  ],\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.12.0\",\n",
      "  \"use_weighted_layer_sum\": false,\n",
      "  \"vocab_size\": 1205,\n",
      "  \"xvector_output_dim\": 512\n",
      "}\n",
      "\n",
      "Adding <s> to the vocabulary\n",
      "Adding </s> to the vocabulary\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "loading configuration file https://huggingface.co/kresnik/wav2vec2-large-xlsr-korean/resolve/main/config.json from cache at C:\\Users\\Jws/.cache\\huggingface\\transformers\\d33b22e404661c9d64ae19906d25af36ff93cb444de2ca532ee5f68ebe79440d.de221d5718b1871c71ff30c71be8d85c0e7038148801458ad4ee9b4a4a6e92e7\n",
      "Model config Wav2Vec2Config {\n",
      "  \"_name_or_path\": \"facebook/wav2vec2-large-xlsr-53\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"adapter_kernel_size\": 3,\n",
      "  \"adapter_stride\": 2,\n",
      "  \"add_adapter\": false,\n",
      "  \"apply_spec_augment\": true,\n",
      "  \"architectures\": [\n",
      "    \"Wav2Vec2ForCTC\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"classifier_proj_size\": 256,\n",
      "  \"codevector_dim\": 768,\n",
      "  \"contrastive_logits_temperature\": 0.1,\n",
      "  \"conv_bias\": true,\n",
      "  \"conv_dim\": [\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512\n",
      "  ],\n",
      "  \"conv_kernel\": [\n",
      "    10,\n",
      "    3,\n",
      "    3,\n",
      "    3,\n",
      "    3,\n",
      "    2,\n",
      "    2\n",
      "  ],\n",
      "  \"conv_stride\": [\n",
      "    5,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2\n",
      "  ],\n",
      "  \"ctc_loss_reduction\": \"mean\",\n",
      "  \"ctc_zero_infinity\": false,\n",
      "  \"diversity_loss_weight\": 0.1,\n",
      "  \"do_stable_layer_norm\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"feat_extract_activation\": \"gelu\",\n",
      "  \"feat_extract_dropout\": 0.0,\n",
      "  \"feat_extract_norm\": \"layer\",\n",
      "  \"feat_proj_dropout\": 0.0,\n",
      "  \"feat_quantizer_dropout\": 0.0,\n",
      "  \"final_dropout\": 0.0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"layerdrop\": 0.1,\n",
      "  \"mask_channel_length\": 10,\n",
      "  \"mask_channel_min_space\": 1,\n",
      "  \"mask_channel_other\": 0.0,\n",
      "  \"mask_channel_prob\": 0.0,\n",
      "  \"mask_channel_selection\": \"static\",\n",
      "  \"mask_feature_length\": 10,\n",
      "  \"mask_feature_min_masks\": 0,\n",
      "  \"mask_feature_prob\": 0.0,\n",
      "  \"mask_time_length\": 10,\n",
      "  \"mask_time_min_masks\": 2,\n",
      "  \"mask_time_min_space\": 1,\n",
      "  \"mask_time_other\": 0.0,\n",
      "  \"mask_time_prob\": 0.05,\n",
      "  \"mask_time_selection\": \"static\",\n",
      "  \"model_type\": \"wav2vec2\",\n",
      "  \"num_adapter_layers\": 3,\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_codevector_groups\": 2,\n",
      "  \"num_codevectors_per_group\": 320,\n",
      "  \"num_conv_pos_embedding_groups\": 16,\n",
      "  \"num_conv_pos_embeddings\": 128,\n",
      "  \"num_feat_extract_layers\": 7,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_negatives\": 100,\n",
      "  \"output_hidden_size\": 1024,\n",
      "  \"pad_token_id\": 1204,\n",
      "  \"proj_codevector_dim\": 768,\n",
      "  \"tdnn_dilation\": [\n",
      "    1,\n",
      "    2,\n",
      "    3,\n",
      "    1,\n",
      "    1\n",
      "  ],\n",
      "  \"tdnn_dim\": [\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    1500\n",
      "  ],\n",
      "  \"tdnn_kernel\": [\n",
      "    5,\n",
      "    3,\n",
      "    3,\n",
      "    1,\n",
      "    1\n",
      "  ],\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.12.0\",\n",
      "  \"use_weighted_layer_sum\": false,\n",
      "  \"vocab_size\": 1205,\n",
      "  \"xvector_output_dim\": 512\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/kresnik/wav2vec2-large-xlsr-korean/resolve/main/pytorch_model.bin from cache at C:\\Users\\Jws/.cache\\huggingface\\transformers\\ebbcc2543e0ca62c918953ec45f1ea748cad86d6e55e1bd81a760e7fec2f8af3.367028c691599b78cd1e381c6cb15b6463f67405be6316365e5eca4242f600b3\n",
      "All model checkpoint weights were used when initializing Wav2Vec2ForCTC.\n",
      "\n",
      "All the weights of Wav2Vec2ForCTC were initialized from the model checkpoint at kresnik/wav2vec2-large-xlsr-korean.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use Wav2Vec2ForCTC for predictions without further training.\n",
      "100%|██████████| 321/321 [00:56<00:00,  5.67ex/s]\n"
     ]
    }
   ],
   "source": [
    "processor = Wav2Vec2Processor.from_pretrained(\"kresnik/wav2vec2-large-xlsr-korean\")\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\"kresnik/wav2vec2-large-xlsr-korean\")\n",
    "\n",
    "def prepare_dataset(batch):\n",
    "    audio_path = f\"../data/reduce/dev/{batch['audio_filename']}\"\n",
    "    speech_array, sampling_rate = torchaudio.load(audio_path)\n",
    "    batch[\"input_values\"] = processor(speech_array.squeeze(), sampling_rate=sampling_rate).input_values\n",
    "    batch[\"labels\"] = np.full(len(batch[\"input_values\"]), batch[\"label\"])\n",
    "    return batch\n",
    "\n",
    "# 데이터셋을 생성\n",
    "data_items = []\n",
    "for participant_id, files in wav_files.items():\n",
    "    label = participant_labels.loc[participant_id, 'PHQ8_Binary']\n",
    "    for file_name in files:\n",
    "        data_items.append({'audio_filename': file_name, 'label': label})\n",
    "\n",
    "dataset = Dataset.from_pandas(pd.DataFrame(data_items))\n",
    "processed_dataset = dataset.map(prepare_dataset, remove_columns=dataset.column_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 학습 파라미터 설정 및 Fine-tuning\n",
    "학습 파라미터를 설정하고, Trainer 객체를 사용하여 모델을 학습시킵니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    per_device_train_batch_size=8,\n",
    "    gradient_accumulation_steps=2,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    num_train_epochs=3,\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir='./logs',\n",
    "    learning_rate=1e-4\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=processed_dataset,\n",
    "    tokenizer=processor.feature_extractor\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
