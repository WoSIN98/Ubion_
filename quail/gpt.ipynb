{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time \n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import base64\n",
    "import concurrent.futures\n",
    "\n",
    "def scrape_data(url):\n",
    "    data = {\n",
    "        'merchant' : [],\n",
    "        'category' : [],\n",
    "        'star' : [],\n",
    "        'address' : [],\n",
    "        'oper_time' : [],\n",
    "        'starCount' : [],\n",
    "        'reviewCount' : []\n",
    "    }\n",
    "    data2 = {\n",
    "        'user_name' : [],\n",
    "        'user_rank' : [],\n",
    "        'num_response' : [],\n",
    "        'user_star' : [], \n",
    "        'time' : [],\n",
    "        'rating' : [],\n",
    "        'content' : []\n",
    "    }\n",
    "\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.get(url)\n",
    "    driver.implicitly_wait(3)\n",
    "\n",
    "    # 영업시간 더 보기 클릭\n",
    "    try:\n",
    "        oper_more = driver.find_elements(By.CSS_SELECTOR, 'a[data-logevent = \"main_info,more_timeinfo\"]')\n",
    "        oper_more[0].click()\n",
    "        time.sleep(2)  \n",
    "\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # 후기 더보기 클릭\n",
    "    try:\n",
    "        link_more = driver.find_element(By.CLASS_NAME, 'txt_more')\n",
    "        while link_more.text == \"후기 더보기\":\n",
    "            link_more = driver.find_element(By.CLASS_NAME, 'txt_more')\n",
    "            if link_more.text != \"후기 접기\":\n",
    "                link_more.click()\n",
    "                time.sleep(0.3)\n",
    "    except:\n",
    "        pass\n",
    "    time.sleep(0.1)\n",
    "  \n",
    "    soup = bs(driver.page_source, 'html.parser')\n",
    "\n",
    "    test_image = driver.execute_script(\"return document.querySelectorAll('canvas')[1].toDataURL();\")\n",
    "    image_data = test_image.split(',')[1]\n",
    "    image_data_decoded = base64.b64decode(image_data)\n",
    "\n",
    "    test_image2 = driver.execute_script(\"return document.querySelectorAll('canvas')[2].toDataURL();\")\n",
    "    image_data2 = test_image2.split(',')[1]\n",
    "    image_data_decoded2 = base64.b64decode(image_data2)\n",
    "\n",
    "    driver.close()\n",
    "\n",
    "    mer = soup.find('h2', {'class':'tit_location'}).get_text()\n",
    "    data['merchant'].append(mer)\n",
    "    data['category'].append(soup.find('span',{'class':'txt_location'}).get_text().split(':')[1].strip())\n",
    "    data['address'].append(soup.find('span', {'class': 'txt_address'}).get_text().replace(\" \", \"\").replace(\"\\n\", \"\"))\n",
    "\n",
    "    if len(soup.find_all('span',{'class':'color_b'})) > 1:\n",
    "        data['star'].append(soup.find_all('span', {'class':'color_b'})[0].get_text().strip('점'))\n",
    "        data['starCount'].append(soup.find('a', {'class': 'link_evaluation'})['data-cnt'])\n",
    "        data['reviewCount'].append(soup.find_all('span', {'class':'color_b'})[1].get_text().strip('개'))\n",
    "    else:\n",
    "        data['star'].append('0')\n",
    "        data['starCount'].append('0')\n",
    "        data['reviewCount'].append(soup.find('span', {'class':'color_b'}).get_text().strip('개'))\n",
    "\n",
    "    if oper_more:    \n",
    "        data['oper_time'].append(soup.find_all('ul', {'class':'list_operation'})[1].get_text().strip())\n",
    "    else:\n",
    "        data['oper_time'].append(soup.find('ul', {'class':'list_operation'}).get_text().strip().replace(\"\\n\", \"\"))\n",
    "\n",
    "    reveiw_listed = soup.find_all('li', {'data-ismy':'false'})\n",
    "\n",
    "    for j in range(len(reveiw_listed)):\n",
    "        data2['user_name'].append(soup.find_all('span',{'class':'txt_username'})[j].get_text())\n",
    "        data2['user_rank'].append(soup.find_all('span', {'class' : 'txt_badge'})[j].get_text().strip('레벨'))\n",
    "        data2['num_response'].append(\n",
    "            soup.find_all('div',{'class':'unit_info'})[j].find_all('span',{'class':'txt_desc'})[0].get_text()\n",
    "        )\n",
    "        data2['user_star'].append(\n",
    "            soup.find_all('div',{'class':'unit_info'})[j].find_all('span',{'class':'txt_desc'})[1].get_text()\n",
    "        )\n",
    "        data2['time'].append(\n",
    "            soup.find_all('div',{'class':'unit_info'})[j].find('span',{'class':'time_write'}).get_text()\n",
    "        )\n",
    "        data2['content'].append(\n",
    "            soup.find_all('p', {'class':'txt_comment'})[j].get_text().strip('더보기')\n",
    "        )\n",
    "\n",
    "        style_attribute = soup.find_all('div', {'class':'star_info'})[j].find('span',{'class':'ico_star inner_star'})['style']\n",
    "        width_percent = float(style_attribute.split(':')[1].strip('%;'))\n",
    "        rating_out_of_five = width_percent / 20  \n",
    "        data2['rating'].append(rating_out_of_five)\n",
    "\n",
    "    df2 = pd.DataFrame(data2)\n",
    "    df2.to_csv(f'./data2/{mer}.csv', index=False)\n",
    "\n",
    "    with open(f'./image/{mer}1.png', 'wb') as f:\n",
    "        f.write(image_data_decoded)\n",
    "\n",
    "    with open(f'./image/{mer}2.png', 'wb') as f:\n",
    "        f.write(image_data_decoded2)\n",
    "\n",
    "    print(f\"{mer}.csv 및 이미지 저장 완료\")\n",
    "\n",
    "    return data\n",
    "\n",
    "def main():\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.get('https://map.kakao.com/')\n",
    "    time.sleep(2)\n",
    "\n",
    "    searchbox = driver.find_element(By.ID, 'search.keyword.query')\n",
    "    searchbox.send_keys('홍대 맛집'+ Keys.ENTER)\n",
    "    time.sleep(3)\n",
    "\n",
    "    list_url = []\n",
    "\n",
    "    driver.find_element(By.ID, 'info.search.place.more').click()\n",
    "    time.sleep(2)\n",
    "    driver.find_element(By.ID, 'info.search.page.no1').click()\n",
    "    time.sleep(2)\n",
    "\n",
    "    for count in range(1):\n",
    "        for inx in range(2, 6):\n",
    "            moreview_element = driver.find_elements(By.CSS_SELECTOR, 'a[data-id=\"moreview\"]')\n",
    "            for i in range(len(moreview_element)):\n",
    "                list_url.append(moreview_element[i].get_attribute('href'))\n",
    "            driver.find_element(By.ID, f'info.search.page.no{inx}').click()\n",
    "            if inx == 5:\n",
    "                moreview_element = driver.find_elements(By.CSS_SELECTOR, 'a[data-id=\"moreview\"]')\n",
    "                for i in range(len(moreview_element)):\n",
    "                    list_url.append(moreview_element[i].get_attribute('href'))\n",
    "            time.sleep(2)\n",
    "        driver.find_element(By.ID ,'info.search.page.next').click()\n",
    "        time.sleep(2)\n",
    "\n",
    "    driver.close()\n",
    "\n",
    "    data_list = []\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        results = executor.map(scrape_data, list_url)\n",
    "        for result in results:\n",
    "            data_list.append(result)\n",
    "\n",
    "    df = pd.DataFrame(data_list)\n",
    "    df.to_csv(\"./data1/data.csv\", index=False)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time \n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import base64\n",
    "import concurrent.futures\n",
    "\n",
    "\n",
    "def scrape_data(url):\n",
    "    data = {\n",
    "        'merchant' : [],\n",
    "        'category' : [],\n",
    "        'star' : [],\n",
    "        'address' : [],\n",
    "        'oper_time' : [],\n",
    "        'starCount' : [],\n",
    "        'reviewCount' : []\n",
    "    }\n",
    "    data2 = {\n",
    "        'user_name' : [],\n",
    "        'user_rank' : [],\n",
    "        'num_response' : [],\n",
    "        'user_star' : [], \n",
    "        'time' : [],\n",
    "        'rating' : [],\n",
    "        'content' : []\n",
    "    }\n",
    "\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.get(url)\n",
    "    driver.implicitly_wait(3)\n",
    "\n",
    "    # 영업시간 더 보기 클릭\n",
    "    try:\n",
    "        oper_more = driver.find_elements(By.CSS_SELECTOR, 'a[data-logevent = \"main_info,more_timeinfo\"]')\n",
    "        oper_more[0].click()\n",
    "        time.sleep(2)  \n",
    "\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # 후기 더보기 클릭\n",
    "    try:\n",
    "        link_more = driver.find_element(By.CLASS_NAME, 'txt_more')\n",
    "        while link_more.text == \"후기 더보기\":\n",
    "            link_more = driver.find_element(By.CLASS_NAME, 'txt_more')\n",
    "            if link_more.text != \"후기 접기\":\n",
    "                link_more.click()\n",
    "                time.sleep(0.3)\n",
    "    except:\n",
    "        pass\n",
    "    time.sleep(0.1)\n",
    "  \n",
    "    soup = bs(driver.page_source, 'html.parser')\n",
    "\n",
    "    try:\n",
    "        test_image = driver.execute_script(\"return document.querySelectorAll('canvas')[1].toDataURL();\")\n",
    "        image_data = test_image.split(',')[1]\n",
    "        image_data_decoded = base64.b64decode(image_data)\n",
    "\n",
    "        test_image2 = driver.execute_script(\"return document.querySelectorAll('canvas')[2].toDataURL();\")\n",
    "        image_data2 = test_image2.split(',')[1]\n",
    "        image_data_decoded2 = base64.b64decode(image_data2)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    try:    \n",
    "        print(driver.find_elements(By.CSS_SELECTOR, 'h2[class = \"tit_location\"]')[1].text,\"파싱완료\")    \n",
    "    except:\n",
    "        print('예외')\n",
    "    driver.close()\n",
    "\n",
    "\n",
    "    mer = soup.find('h2', {'class':'tit_location'}).get_text()\n",
    "    data['merchant'].append(mer)\n",
    "    data['category'].append(soup.find('span',{'class':'txt_location'}).get_text().split(':')[1].strip())\n",
    "    data['address'].append(soup.find('span', {'class': 'txt_address'}).get_text().replace(\" \", \"\").replace(\"\\n\", \"\"))\n",
    "\n",
    "    if len(soup.find_all('span',{'class':'color_b'})) > 1:\n",
    "        data['star'].append(soup.find_all('span', {'class':'color_b'})[0].get_text().strip('점'))\n",
    "        data['starCount'].append(soup.find('a', {'class': 'link_evaluation'})['data-cnt'])\n",
    "        data['reviewCount'].append(soup.find_all('span', {'class':'color_b'})[1].get_text().strip('개'))\n",
    "    else:\n",
    "        data['star'].append('0')\n",
    "        data['starCount'].append('0')\n",
    "        data['reviewCount'].append(soup.find('span', {'class':'color_b'}).get_text().strip('개'))\n",
    "\n",
    "    if oper_more:    \n",
    "        data['oper_time'].append(soup.find_all('ul', {'class':'list_operation'})[1].get_text().strip())\n",
    "    else:\n",
    "        data['oper_time'].append(soup.find('ul', {'class':'list_operation'}).get_text().strip().replace(\"\\n\", \"\"))\n",
    "\n",
    "    reveiw_listed = soup.find_all('li', {'data-ismy':'false'})\n",
    "\n",
    "    for j in range(len(reveiw_listed)):\n",
    "        data2['user_name'].append(soup.find_all('span',{'class':'txt_username'})[j].get_text())\n",
    "        data2['user_rank'].append(soup.find_all('span', {'class' : 'txt_badge'})[j].get_text().strip('레벨'))\n",
    "        data2['num_response'].append(\n",
    "            soup.find_all('div',{'class':'unit_info'})[j].find_all('span',{'class':'txt_desc'})[0].get_text()\n",
    "        )\n",
    "        data2['user_star'].append(\n",
    "            soup.find_all('div',{'class':'unit_info'})[j].find_all('span',{'class':'txt_desc'})[1].get_text()\n",
    "        )\n",
    "        data2['time'].append(\n",
    "            soup.find_all('div',{'class':'unit_info'})[j].find('span',{'class':'time_write'}).get_text()\n",
    "        )\n",
    "        data2['content'].append(\n",
    "            soup.find_all('p', {'class':'txt_comment'})[j].get_text().strip('더보기')\n",
    "        )\n",
    "\n",
    "        style_attribute = soup.find_all('div', {'class':'star_info'})[j].find('span',{'class':'ico_star inner_star'})['style']\n",
    "        width_percent = float(style_attribute.split(':')[1].strip('%;'))\n",
    "        rating_out_of_five = width_percent / 20  \n",
    "        data2['rating'].append(rating_out_of_five)\n",
    "\n",
    "    df2 = pd.DataFrame(data2)\n",
    "    df2.to_csv(f'./data2/{mer}.csv', index=False)\n",
    "\n",
    "    try:\n",
    "        with open(f'./image/{mer}1.png', 'wb') as f:\n",
    "            f.write(image_data_decoded)\n",
    "\n",
    "        with open(f'./image/{mer}2.png', 'wb') as f:\n",
    "            f.write(image_data_decoded2)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    print(f\"{mer}.csv 및 이미지 저장 완료\")\n",
    "\n",
    "    return data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "음식점 개수 :  225\n",
      "강강술래 홍대점 파싱완료\n",
      "강강술래 홍대점.csv 및 이미지 저장 완료\n",
      "또보겠지떡볶이집 깐따삐아점 파싱완료\n",
      "또보겠지떡볶이집 깐따삐아점.csv 및 이미지 저장 완료\n",
      "감성타코 홍대점 파싱완료\n",
      "우와 홍대본점 파싱완료\n",
      "맛이차이나 파싱완료\n",
      "비스트로주라 파싱완료\n",
      "카미야 파싱완료\n",
      "땡스네이쳐 파싱완료\n",
      "피오니 홍대점 파싱완료\n",
      "칸다소바 홍대점 파싱완료\n",
      "943킹스크로스 파싱완료\n",
      "땡스네이쳐.csv 및 이미지 저장 완료\n",
      "감성타코 홍대점.csv 및 이미지 저장 완료\n",
      "홍대 조선시대 파싱완료\n",
      "맛이차이나.csv 및 이미지 저장 완료\n",
      "프리모바치오바치 홍대본점 파싱완료\n",
      "하카타분코 파싱완료\n",
      "우와 홍대본점.csv 및 이미지 저장 완료\n",
      "카미야.csv 및 이미지 저장 완료\n",
      "비스트로주라.csv 및 이미지 저장 완료\n",
      "프리모바치오바치 홍대본점.csv 및 이미지 저장 완료\n",
      "진진 파싱완료\n",
      "테이스티버거 파싱완료\n",
      "가미우동 파싱완료\n",
      "진진.csv 및 이미지 저장 완료\n",
      "츠케루 파싱완료\n",
      "정돈 파싱완료\n",
      "피오니 홍대점.csv 및 이미지 저장 완료\n",
      "여우골 홍대점 파싱완료\n",
      "칸다소바 홍대점.csv 및 이미지 저장 완료\n",
      "하이디라오 홍대지점 파싱완료\n",
      "마녀주방 홍대점 파싱완료\n",
      "테이스티버거.csv 및 이미지 저장 완료\n",
      "마녀주방 홍대점.csv 및 이미지 저장 완료\n",
      "콜린 파싱완료\n",
      "여우골 홍대점.csv 및 이미지 저장 완료\n",
      "하카타분코.csv 및 이미지 저장 완료\n",
      "공감 홍대2호점 파싱완료\n",
      "공감 홍대2호점.csv 및 이미지 저장 완료\n",
      "산더미불고기 파싱완료\n",
      "콜린.csv 및 이미지 저장 완료\n",
      "산더미불고기.csv 및 이미지 저장 완료\n",
      "어반플랜트 합정 파싱완료\n",
      "개화기요정 파싱완료\n",
      "비스트로사랑방 파싱완료\n",
      "하이디라오 홍대지점.csv 및 이미지 저장 완료\n",
      "개화기요정.csv 및 이미지 저장 완료\n",
      "비스트로사랑방.csv 및 이미지 저장 완료\n",
      "더담 파싱완료\n",
      "정돈.csv 및 이미지 저장 완료더담.csv 및 이미지 저장 완료\n",
      "\n",
      "짬뽕지존 홍대점 파싱완료\n",
      "이츠모라멘 파싱완료\n",
      "이츠모라멘.csv 및 이미지 저장 완료\n",
      "짬뽕지존 홍대점.csv 및 이미지 저장 완료\n",
      "장인닭갈비 홍대점 파싱완료\n",
      "장인닭갈비 홍대점.csv 및 이미지 저장 완료\n",
      "아오이토리 파싱완료\n",
      "냉장고 파싱완료\n",
      "삼거리포차 파싱완료\n",
      "미어캣프랜즈 파싱완료\n",
      "미어캣프랜즈.csv 및 이미지 저장 완료\n",
      "오레노라멘 합정본점 파싱완료\n",
      "냉장고.csv 및 이미지 저장 완료\n",
      "홍마떡 홍대본점 파싱완료\n",
      "홍마떡 홍대본점.csv 및 이미지 저장 완료\n",
      "지로우라멘 파싱완료\n",
      "소코아 홍대점 파싱완료\n",
      "943킹스크로스.csv 및 이미지 저장 완료\n",
      "아오이토리.csv 및 이미지 저장 완료\n",
      "가미우동.csv 및 이미지 저장 완료\n",
      "김덕후의곱창조 홍대본점 파싱완료\n",
      "소코아 홍대점.csv 및 이미지 저장 완료\n",
      "스아게K 파싱완료\n",
      "델리인디아 파싱완료\n",
      "이치류 홍대본점 파싱완료\n",
      "피자네버슬립스 합정상수점 파싱완료\n",
      "황곱 홍대점 파싱완료\n",
      "이치류 홍대본점.csv 및 이미지 저장 완료\n",
      "어반플랜트 합정.csv 및 이미지 저장 완료\n",
      "델리인디아.csv 및 이미지 저장 완료\n",
      "혼가츠 파싱완료\n",
      "스탠스커피 파싱완료\n",
      "버터밀크 파싱완료\n",
      "혼가츠.csv 및 이미지 저장 완료\n",
      "스탠스커피.csv 및 이미지 저장 완료\n",
      "하하&김종국의 401정육식당 홍대본점 파싱완료\n",
      "스아게K.csv 및 이미지 저장 완료\n",
      "하하&김종국의 401정육식당 홍대본점.csv 및 이미지 저장 완료\n",
      "이빠네마그릴 홍대점 파싱완료\n",
      "산울림1992 파싱완료\n",
      "이빠네마그릴 홍대점.csv 및 이미지 저장 완료\n",
      "멘야산다이메 홍대점 파싱완료\n",
      "광동포차 파싱완료\n",
      "광동포차.csv 및 이미지 저장 완료\n",
      "홍대인파스타 파싱완료\n",
      "산울림1992.csv 및 이미지 저장 완료\n",
      "홍대인파스타.csv 및 이미지 저장 완료\n",
      "멘야산다이메 홍대점.csv 및 이미지 저장 완료\n",
      "웃사브 파싱완료\n",
      "후라토식당 상수직영점 파싱완료\n",
      "아웃닭 홍대점 파싱완료\n",
      "아웃닭 홍대점.csv 및 이미지 저장 완료\n",
      "후라토식당 상수직영점.csv 및 이미지 저장 완료\n",
      "백년토종삼계탕 본점 파싱완료\n",
      "효봉포차 파싱완료\n",
      "웃사브.csv 및 이미지 저장 완료\n",
      "지로우라멘.csv 및 이미지 저장 완료\n",
      "백년토종삼계탕 본점.csv 및 이미지 저장 완료\n",
      "이치류 홍대본점 파싱완료\n",
      "버터밀크.csv 및 이미지 저장 완료\n",
      "스탠스커피 파싱완료\n",
      "혼가츠 파싱완료\n",
      "버터밀크 파싱완료\n",
      "하하&김종국의 401정육식당 홍대본점 파싱완료\n",
      "피자네버슬립스 합정상수점.csv 및 이미지 저장 완료\n",
      "스탠스커피.csv 및 이미지 저장 완료\n",
      "하하&김종국의 401정육식당 홍대본점.csv 및 이미지 저장 완료\n",
      "이빠네마그릴 홍대점 파싱완료\n",
      "산울림1992 파싱완료\n",
      "이치류 홍대본점.csv 및 이미지 저장 완료\n",
      "이빠네마그릴 홍대점.csv 및 이미지 저장 완료\n",
      "광동포차 파싱완료\n",
      "광동포차.csv 및 이미지 저장 완료\n",
      "혼가츠.csv 및 이미지 저장 완료\n",
      "멘야산다이메 홍대점 파싱완료\n",
      "홍대인파스타 파싱완료\n",
      "홍대인파스타.csv 및 이미지 저장 완료\n",
      "후라토식당 상수직영점 파싱완료\n",
      "웃사브 파싱완료\n",
      "산울림1992.csv 및 이미지 저장 완료\n",
      "아웃닭 홍대점 파싱완료\n",
      "아웃닭 홍대점.csv 및 이미지 저장 완료\n",
      "백년토종삼계탕 본점 파싱완료\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.get('https://map.kakao.com/')\n",
    "    time.sleep(2)\n",
    "\n",
    "    searchbox = driver.find_element(By.ID, 'search.keyword.query')\n",
    "    searchbox.send_keys('홍대 맛집'+ Keys.ENTER)\n",
    "    time.sleep(3)\n",
    "\n",
    "    list_url = []\n",
    "\n",
    "    driver.find_element(By.ID, 'info.search.place.more').click()\n",
    "    time.sleep(2)\n",
    "    driver.find_element(By.ID, 'info.search.page.no1').click()\n",
    "    time.sleep(2)\n",
    "\n",
    "    for count in range(3):\n",
    "        for inx in range(2, 6):\n",
    "            moreview_element = driver.find_elements(By.CSS_SELECTOR, 'a[data-id=\"moreview\"]')\n",
    "            for i in range(len(moreview_element)):\n",
    "                list_url.append(moreview_element[i].get_attribute('href'))\n",
    "            driver.find_element(By.ID, f'info.search.page.no{inx}').click()\n",
    "            if inx == 5:\n",
    "                moreview_element = driver.find_elements(By.CSS_SELECTOR, 'a[data-id=\"moreview\"]')\n",
    "                for i in range(len(moreview_element)):\n",
    "                    list_url.append(moreview_element[i].get_attribute('href'))\n",
    "            time.sleep(2)\n",
    "        driver.find_element(By.ID ,'info.search.page.next').click()\n",
    "        time.sleep(2)\n",
    "        \n",
    "    print(\"음식점 개수 : \", len(list_url))\n",
    "\n",
    "    driver.close()\n",
    "\n",
    "    data_list = []\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        results = executor.map(scrape_data, list_url)\n",
    "        for result in results:\n",
    "            data_list.append(result)\n",
    "\n",
    "    df = pd.DataFrame(data_list)\n",
    "    df.to_csv(\"./data1/data.csv\", index=False)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
